import logging
import os
import random
import vk_api
from vk_api.utils import get_random_id

import openai

from config import VK_GROUP_TOKEN, YANDEX_FOLDER_ID, YANDEX_LLM_API_KEY
import db

logger = logging.getLogger(__name__)

DEFAULT_MAX_OUTPUT_TOKENS = 300
DEFAULT_MODEL= 'yandexgpt/rc'
DEFAULT_TEMPERATURE = 0.9

# ---------------------------------------------------------------------------
# Load prompts once at module import time.
# In the deployed function files live at prompts/<name> relative to cwd;
# fall back to the repo path for tests.
# ---------------------------------------------------------------------------
def _load_prompt(filename: str) -> str:
    paths = [
        os.path.join(os.path.dirname(__file__), "prompts", filename),
        os.path.join(os.path.dirname(__file__), "..", "src", "prompts", filename),
    ]
    for p in paths:
        if os.path.exists(p):
            with open(p, encoding="utf-8") as f:
                return f.read().strip()
    return ""

GEESE_STORY_PROMPT = _load_prompt("geese_story_prompt.txt")
WHO_IS_TODAY_PROMPT = _load_prompt("who_is_today_prompt.txt")
EXPLAIN_PROMPT = _load_prompt("explain_prompt.txt")

# ---------------------------------------------------------------------------
# Token economy constants for –∫—Ç–æ —Å–µ–≥–æ–¥–Ω—è
# ---------------------------------------------------------------------------
# Model context: 32,768 tokens.
# Reserve ~1,000 for system prompt + instruction overhead, ~500 for output.
# Remaining budget for user messages: ~31,000 tokens.
# Rough estimate: 1 token ‚âà 3.5 Russian characters.
_WHO_IS_TODAY_CHAR_BUDGET = 31_000 * 3  # ~93,000 chars total for all users
_WHO_IS_TODAY_MAX_OUTPUT_TOKENS = DEFAULT_MAX_OUTPUT_TOKENS

GEESE_PLACEHOLDER_MESSAGES = [
    "–ù—É —Ö–æ—Ä–æ—à–æ —Ö–æ—Ä–æ—à–æ, —Å–µ–π—á–∞—Å –ø–æ–¥—É–º–∞–µ–º, —á—Ç–æ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å‚Ä¶",
    "–ì—É—Å–∏ —É–∂–µ –≤ –∫—É—Ä—Å–µ. –°–æ–±–∏—Ä–∞–µ–º –º—É–¥—Ä–æ—Å—Ç—å‚Ä¶",
    "–û–¥–∏–Ω –º–æ–º–µ–Ω—Ç, –∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É—é—Å—å —Å–æ —Å—Ç–∞–µ–π‚Ä¶",
    "–õ–∞–¥–Ω–æ, –¥–∞–π —Å–æ–±—Ä–∞—Ç—å—Å—è —Å –º—ã—Å–ª—è–º–∏. –ì—É—Å–∏ –¥—É–º–∞—é—Ç.",
    "–ü—Ä–∏–Ω—è—Ç–æ. –û—Ç–∫—Ä—ã–≤–∞—é –∫–Ω–∏–≥—É –≥—É—Å–∏–Ω–æ–π –º—É–¥—Ä–æ—Å—Ç–∏‚Ä¶",
    "–•–º, –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –∑–∞–ø—Ä–æ—Å. –ì—É—Å–∏ —Å–æ–≤–µ—â–∞—é—Ç—Å—è‚Ä¶",
    "–°–µ–π—á–∞—Å, —Å–µ–π—á–∞—Å. –ì–ª–∞–≤–Ω—ã–π –≥—É—Å—å –±–µ—Ä—ë—Ç —Å–ª–æ–≤–æ‚Ä¶",
    "–û–∫, –∑–∞–ø—Ä–æ—Å –ø—Ä–∏–Ω—è—Ç. –ñ–¥–∏ –≥—É—Å–∏–Ω–æ–≥–æ –æ—Ç–∫—Ä–æ–≤–µ–Ω–∏—è.",
    "–°—Ç–∞—è —Å–æ–±–∏—Ä–∞–µ—Ç—Å—è. –≠—Ç–æ –∑–∞–π–º—ë—Ç —Å–µ–∫—É–Ω–¥—É‚Ä¶",
    "–ú–æ–ª—á—É, –¥—É–º–∞—é, –ø–∏—à—É. –ì—É—Å–∏ –Ω–µ —Ç–æ—Ä–æ–ø—è—Ç—Å—è.",
]

WHO_IS_TODAY_PLACEHOLDER_MESSAGES = [
    "–ò–∑—É—á–∞—é –ø–µ—Ä–µ–ø–∏—Å–∫—É‚Ä¶ —ç—Ç–æ –∑–∞–π–º—ë—Ç —Å–µ–∫—É–Ω–¥—É.",
    "–õ–∏—Å—Ç–∞—é —á–∞—Ç, –∏—â—É –¥–æ—Å—Ç–æ–π–Ω–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞‚Ä¶",
    "–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —É–ª–∏–∫–∏. –ö—Ç–æ-—Ç–æ —Å–µ–≥–æ–¥–Ω—è —è–≤–Ω–æ –æ—Ç–ª–∏—á–∏–ª—Å—è.",
    "–û–¥–∏–Ω –º–æ–º–µ–Ω—Ç, –ø—Ä–æ–≤–æ–∂—É —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ‚Ä¶",
    "–°–µ–π—á–∞—Å —Ä–∞–∑–±–µ—Ä—ë–º—Å—è, –∫—Ç–æ —Ç—É—Ç –≥–µ—Ä–æ–π –¥–Ω—è.",
]

EXPLAIN_PLACEHOLDER_MESSAGES = [
    "–ú–∏–Ω—É—Ç—É, –ø–µ—Ä–µ–≤–∞—Ä–∏–≤–∞—é —Ç–µ–∫—Å—Ç‚Ä¶",
    "–£–∂–µ —á–∏—Ç–∞—é. –°–µ–π—á–∞—Å –æ–±—ä—è—Å–Ω—é.",
    "–•–º, –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ. –ü–µ—Ä–µ–∫–ª–∞–¥—ã–≤–∞—é –Ω–∞ –Ω—É–∂–Ω—ã–π —è–∑—ã–∫‚Ä¶",
    "–û—Å–º—ã—Å–ª—è—é. –ñ–¥–∏.",
    "–°–µ–∫—É–Ω–¥—É ‚Äî –∏—â—É –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Å–ª–æ–≤–∞.",
]

DEFAULT_EXPLAIN_STYLES = [
    "–ø–æ-–ø–∞—Ü–∞–Ω—Å–∫–∏",
    "–∫–∞–∫ –®–µ–∫—Å–ø–∏—Ä",
    "–∫–∞–∫ –¥–∏–∫—Ç–æ—Ä —Å–æ–≤–µ—Ç—Å–∫–æ–≥–æ —Ä–∞–¥–∏–æ",
    "–∫–∞–∫ —Ä—ç–ø–µ—Ä –∏–∑ 90-—Ö",
    "–∫–∞–∫ —É—Å—Ç–∞–≤—à–∏–π —É—á–∏—Ç–µ–ª—å —Ö–∏–º–∏–∏",
    "–∫–∞–∫ —Å—Ç–∞—Ä—Ç–∞–ø–µ—Ä –Ω–∞ –ø–∏—Ç—á–µ",
    "–∫–∞–∫ –±–∞–±—É—à–∫–∞ –Ω–∞ –ª–∞–≤–æ—á–∫–µ",
    "–∫–∞–∫ —Ñ—É—Ç–±–æ–ª—å–Ω—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ç–æ—Ä",
    "–∫–∞–∫ –≤–æ–µ–Ω–Ω—ã–π –Ω–∞ –±—Ä–∏—Ñ–∏–Ω–≥–µ",
    "–∫–∞–∫ —Ñ–∏–ª–æ—Å–æ—Ñ-—ç–∫–∑–∏—Å—Ç–µ–Ω—Ü–∏–∞–ª–∏—Å—Ç",
]


def _get_vk():
    vk_session = vk_api.VkApi(token=VK_GROUP_TOKEN)
    return vk_session.get_api()


def send_message(peer_id, text):
    logger.info("Sending message to peer_id=%s: %s", peer_id, text.replace('\n', ' '))
    _get_vk().messages.send(
        peer_id=peer_id,
        random_id=get_random_id(),
        message=text
    )


def get_user_name(user_id: int) -> str:
    info = _get_vk().users.get(user_ids=user_id)[0]
    return f"{info['first_name']} {info['last_name']}"


def handle_planka(msg, text: str):
    peer_id = msg["peer_id"]
    user_id = msg["from_id"]

    name = get_user_name(user_id)

    actual_seconds = None
    parts = text.strip().split()
    if len(parts) >= 2 and parts[0].lower() == "–ø–ª–∞–Ω–∫–∞":
        try:
            actual_seconds = int(parts[1])
        except ValueError:
            actual_seconds = None

    result = db.mark_plank(user_id, name, actual_seconds)
    today_str = db.get_today_date_str()

    if result.is_new:
        if actual_seconds is not None:
            send_message(peer_id, f"{today_str} –ø–ª–∞–Ω–∫–∞ —Å–¥–µ–ª–∞–Ω–∞ ({actual_seconds})")
        else:
            send_message(peer_id, f"{today_str} –ø–ª–∞–Ω–∫–∞ —Å–¥–µ–ª–∞–Ω–∞")
    elif result.was_updated:
        send_message(peer_id, f"–ø–ª–∞–Ω–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∞ ({actual_seconds}) üí™")
    else:
        send_message(peer_id, "–ø–ª–∞–Ω–∫–∞ —É–∂–µ —Å–¥–µ–ª–∞–Ω–∞")


def handle_stats(msg):
    peer_id = msg["peer_id"]
    done, not_done = db.get_stats_for_today()

    today_str = db.get_today_date_str()
    text_parts = [f"–°—Ç–∞—Ç–∞ –∑–∞ {today_str}:"]

    if done:
        text_parts.append("–°–¥–µ–ª–∞–ª–∏ –ø–ª–∞–Ω–∫—É:")
        text_parts.append(", ".join(done))
    else:
        text_parts.append("–°–¥–µ–ª–∞–ª–∏ –ø–ª–∞–Ω–∫—É: –Ω–∏–∫—Ç–æ")

    if not_done:
        text_parts.append("")
        text_parts.append("–ù–µ —Å–¥–µ–ª–∞–ª–∏ –ø–ª–∞–Ω–∫—É:")
        text_parts.append(", ".join(not_done))
    else:
        text_parts.append("")
        text_parts.append("–í—Å–µ –æ—Ç–º–µ—Ç–∏–ª–∏—Å—å –∏–ª–∏ –µ—â—ë –Ω–∏–∫—Ç–æ –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω –≤ –±–∞–∑—É")

    send_message(peer_id, "\n".join(text_parts))


def handle_guide(msg):
    peer_id = msg["peer_id"]
    text = (
        "–ì–∞–π–¥ –ø–æ –∫–æ–º–∞–Ω–¥–∞–º:\n"
        "‚Ä¢ –ø–ª–∞–Ω–∫–∞ ‚Äî –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ —Ç—ã —Å–¥–µ–ª–∞–ª(–∞) –ø–ª–∞–Ω–∫—É —Å–µ–≥–æ–¥–Ω—è.\n"
        "‚Ä¢ –ø–ª–∞–Ω–∫–∞ X ‚Äî –æ—Ç–º–µ—Ç–∏—Ç—å –ø–ª–∞–Ω–∫—É —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —á–∏—Å–ª–∞ —Å–µ–∫—É–Ω–¥ X.\n"
        "  –ï—Å–ª–∏ –ø–ª–∞–Ω–∫–∞ —É–∂–µ –∑–∞–ø–∏—Å–∞–Ω–∞, –∑–Ω–∞—á–µ–Ω–∏–µ X –æ–±–Ω–æ–≤–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç.\n"
        "‚Ä¢ —Å—Ç–∞—Ç–∞ ‚Äî –ø–æ–∫–∞–∑–∞—Ç—å, –∫—Ç–æ —Å–µ–≥–æ–¥–Ω—è —Å–¥–µ–ª–∞–ª –ø–ª–∞–Ω–∫—É –∏ –∫—Ç–æ –Ω–µ—Ç.\n"
        "‚Ä¢ –≥–∞–π–¥ ‚Äî –ø–æ–∫–∞–∑–∞—Ç—å —ç—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ.\n"
        "‚Ä¢ –µ–±–∞—Ç—å –≥—É—Å–µ–π [–∫–æ–Ω—Ç–µ–∫—Å—Ç] ‚Äî –º—É–¥—Ä–∞—è –∏—Å—Ç–æ—Ä–∏—è –ø—Ä–æ –≥—É—Å–µ–π –∏ –ø–ª–∞–Ω–∫—É.\n"
        "‚Ä¢ –∫—Ç–æ —Å–µ–≥–æ–¥–Ω—è [–≤–æ–ø—Ä–æ—Å] ‚Äî –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–æ–±–µ–¥–∏—Ç–µ–ª—è –¥–Ω—è –ø–æ –ø–µ—Ä–µ–ø–∏—Å–∫–µ –≤ —á–∞—Ç–µ.\n"
        "  –ù–∞–ø—Ä–∏–º–µ—Ä: –∫—Ç–æ —Å–µ–≥–æ–¥–Ω—è –±–æ–ª—å—à–µ –≤—Å–µ—Ö –ø–æ—Ö–æ–∂ –Ω–∞ –¶–æ—è?\n"
        "‚Ä¢ –æ–±—ä—è—Å–Ω–∏ [–∫–∞–∫] ‚Äî –æ–±—ä—è—Å–Ω–∏—Ç—å –ø—Ä–∏–ª–æ–∂–µ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –Ω—É–∂–Ω–æ–º —Å—Ç–∏–ª–µ.\n"
        "  –û—Ç–≤–µ—Ç—å –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–ª–∏ –ø–µ—Ä–µ—à–ª–∏ –µ–≥–æ, –∑–∞—Ç–µ–º –Ω–∞–ø–∏—à–∏ ¬´–æ–±—ä—è—Å–Ω–∏ –ø–æ-–ø–∞—Ü–∞–Ω—Å–∫–∏¬ª.\n"
        "  –ï—Å–ª–∏ —Å—Ç–∏–ª—å –Ω–µ —É–∫–∞–∑–∞–Ω ‚Äî –≤—ã–±–µ—Ä—É —Å–∞–º. –ú–æ–∂–Ω–æ –ø–µ—Ä–µ—Å–ª–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏–π.\n"
    )
    send_message(peer_id, text)


def _call_llm(extra_context: str) -> str:
    """Call Yandex AI Studio (OpenAI-compatible) and return the generated story."""
    client = openai.OpenAI(
        api_key=YANDEX_LLM_API_KEY,
        base_url="https://ai.api.cloud.yandex.net/v1",
        project=YANDEX_FOLDER_ID,
    )
    response = client.responses.create(
        model=f"gpt://{YANDEX_FOLDER_ID}/{DEFAULT_MODEL}",
        temperature=DEFAULT_TEMPERATURE,
        instructions=GEESE_STORY_PROMPT,
        input=extra_context if extra_context else "–ø—Ä–æ—Å—Ç–æ –∏—Å—Ç–æ—Ä–∏—è",
        max_output_tokens=DEFAULT_MAX_OUTPUT_TOKENS,
    )
    return response.output_text


def _build_who_is_today_input(question: str, user_messages: list[tuple[str, list[str]]]) -> str:
    """
    Build the LLM input string for –∫—Ç–æ —Å–µ–≥–æ–¥–Ω—è.

    Applies a fair token economy: the total character budget is divided equally
    among all users. Within each user's quota the most-recent messages are kept
    (reversed order, newest first) so that fresh context survives trimming.

    Args:
        question: the raw question text (e.g. "–±–æ–ª—å—à–µ –≤—Å–µ—Ö –ø–æ—Ö–æ–∂ –Ω–∞ –¶–æ—è")
        user_messages: list of (user_name, [msg_text, ...]) oldest-first

    Returns:
        Formatted string to pass as `input` to the LLM.
    """
    if not user_messages:
        return f"–í–æ–ø—Ä–æ—Å: {question}\n\n–°–æ–æ–±—â–µ–Ω–∏–π –≤ —á–∞—Ç–µ –∑–∞ —Å–µ–≥–æ–¥–Ω—è –Ω–µ—Ç."

    n_users = len(user_messages)
    per_user_budget = _WHO_IS_TODAY_CHAR_BUDGET // n_users

    sections = []
    for name, msgs in user_messages:
        # Take newest messages first until budget is exhausted, then reverse back
        selected: list[str] = []
        remaining = per_user_budget
        for msg in reversed(msgs):
            if remaining <= 0:
                break
            chunk = msg[:remaining]
            selected.append(chunk)
            remaining -= len(chunk)
        selected.reverse()  # restore chronological order

        user_block = "\n".join(f"  ‚Äî {m}" for m in selected)
        sections.append(f"{name}:\n{user_block}")

    messages_block = "\n\n".join(sections)
    return (
        f"–í–æ–ø—Ä–æ—Å: {question}\n\n"
        f"–ü–µ—Ä–µ–ø–∏—Å–∫–∞ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –∑–∞ —Å–µ–≥–æ–¥–Ω—è:\n\n"
        f"{messages_block}"
    )


def _call_explain_llm(text_to_explain: str, style: str) -> str:
    """Call LLM to explain text_to_explain in the given style."""
    client = openai.OpenAI(
        api_key=YANDEX_LLM_API_KEY,
        base_url="https://ai.api.cloud.yandex.net/v1",
        project=YANDEX_FOLDER_ID,
    )
    llm_input = f"–°—Ç–∏–ª—å –æ–±—ä—è—Å–Ω–µ–Ω–∏—è: {style}\n\n–¢–µ–∫—Å—Ç:\n{text_to_explain}"
    response = client.responses.create(
        model=f"gpt://{YANDEX_FOLDER_ID}/{DEFAULT_MODEL}",
        temperature=DEFAULT_TEMPERATURE,
        instructions=EXPLAIN_PROMPT,
        input=llm_input,
        max_output_tokens=DEFAULT_MAX_OUTPUT_TOKENS,
    )
    return response.output_text


def handle_explain(msg, text_raw: str):
    """
    Handle "–æ–±—ä—è—Å–Ω–∏ [–∫–∞–∫]" command.

    Extracts the text to explain from reply_message or fwd_messages,
    then calls LLM to explain it in the requested style.
    """
    peer_id = msg["peer_id"]

    # Step 1 ‚Äî extract text to explain
    source_text = None

    reply = msg.get("reply_message")
    if reply and reply.get("text"):
        source_text = reply["text"]
    else:
        fwd = msg.get("fwd_messages") or []
        texts = [m["text"] for m in fwd if m.get("text")]
        if texts:
            source_text = "\n\n".join(texts)

    if not source_text:
        send_message(
            peer_id,
            "–û—Ç–≤–µ—Ç—å –Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–ª–∏ –ø–µ—Ä–µ—à–ª–∏ –µ–≥–æ, –∞ –ø–æ—Ç–æ–º –Ω–∞–ø–∏—à–∏ ¬´–æ–±—ä—è—Å–Ω–∏ [–∫–∞–∫]¬ª."
        )
        return

    # Step 2 ‚Äî extract style
    trigger = "–æ–±—ä—è—Å–Ω–∏"
    lower_raw = text_raw.lower()
    idx = lower_raw.find(trigger)
    if idx != -1:
        style = text_raw[idx + len(trigger):].strip()
    else:
        style = ""

    if not style:
        style = random.choice(DEFAULT_EXPLAIN_STYLES)

    logger.info("handle_explain: style=%r source_text=%r", style, source_text[:80])

    # Step 3 ‚Äî send placeholder, then call LLM
    send_message(peer_id, random.choice(EXPLAIN_PLACEHOLDER_MESSAGES))

    try:
        explanation = _call_explain_llm(source_text, style)
    except Exception as e:
        logger.error("LLM call failed for explain: %s", e)
        send_message(peer_id, "–ß—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫. –ü–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ.")
        return

    send_message(peer_id, explanation)


def _call_who_is_today_llm(question: str, user_messages: list[tuple[str, list[str]]]) -> str:
    """Call LLM for –∫—Ç–æ —Å–µ–≥–æ–¥–Ω—è command."""
    client = openai.OpenAI(
        api_key=YANDEX_LLM_API_KEY,
        base_url="https://ai.api.cloud.yandex.net/v1",
        project=YANDEX_FOLDER_ID,
    )
    llm_input = _build_who_is_today_input(question, user_messages)
    response = client.responses.create(
        model=f"gpt://{YANDEX_FOLDER_ID}/{DEFAULT_MODEL}",
        temperature=DEFAULT_TEMPERATURE,
        instructions=WHO_IS_TODAY_PROMPT,
        input=llm_input,
        max_output_tokens=_WHO_IS_TODAY_MAX_OUTPUT_TOKENS,
    )
    return response.output_text


def handle_geese(msg, text_raw: str):
    """
    Handle the "–µ–±–∞—Ç—å –≥—É—Å–µ–π [extra context]" command.

    1. Immediately dispatch a random placeholder message.
    2. Parse extra context (everything after "–µ–±–∞—Ç—å –≥—É—Å–µ–π").
    3. Call the LLM and send the resulting story.
    """
    peer_id = msg["peer_id"]

    # Step 1 ‚Äî send placeholder immediately
    send_message(peer_id, random.choice(GEESE_PLACEHOLDER_MESSAGES))

    # Step 2 ‚Äî extract extra context
    trigger = "–µ–±–∞—Ç—å –≥—É—Å–µ–π"
    lower_raw = text_raw.lower()
    idx = lower_raw.find(trigger)
    if idx != -1:
        extra_context = text_raw[idx + len(trigger):].strip()
    else:
        extra_context = ""

    logger.info("handle_geese: extra_context=%r", extra_context)

    # Step 3 ‚Äî call LLM and send story
    try:
        story = _call_llm(extra_context)
    except Exception as e:
        logger.error("LLM call failed: %s", e)
        send_message(peer_id, "–ì—É—Å–∏ –º–æ–ª—á–∞—Ç. –ß—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫ –Ω–∞ –∏—Ö —Å—Ç–æ—Ä–æ–Ω–µ.")
        return

    send_message(peer_id, story + "\n\n–í—ã –µ–±–µ—Ç–µ –≥—É—Å–µ–π.")


def handle_who_is_today(msg, text_raw: str):
    """
    Handle "–∫—Ç–æ —Å–µ–≥–æ–¥–Ω—è [question]" command.

    1. Send a placeholder immediately.
    2. Extract the question (everything after "–∫—Ç–æ —Å–µ–≥–æ–¥–Ω—è").
    3. Load today's messages from DB.
    4. Apply token economy and call LLM.
    5. Send the result.
    """
    peer_id = msg["peer_id"]

    # Step 1 ‚Äî extract question
    trigger = "–∫—Ç–æ —Å–µ–≥–æ–¥–Ω—è"
    lower_raw = text_raw.lower()
    idx = lower_raw.find(trigger)
    if idx != -1:
        question = text_raw[idx + len(trigger):].strip()
    else:
        question = ""

    if not question:
        send_message(peer_id, "–£–∫–∞–∂–∏ –≤–æ–ø—Ä–æ—Å. –ù–∞–ø—Ä–∏–º–µ—Ä: –∫—Ç–æ —Å–µ–≥–æ–¥–Ω—è –±–æ–ª—å—à–µ –≤—Å–µ—Ö –ø–æ—Ö–æ–∂ –Ω–∞ –¶–æ—è?")
        return

    logger.info("handle_who_is_today: question=%r", question)

    # Step 2 ‚Äî send placeholder
    send_message(peer_id, random.choice(WHO_IS_TODAY_PLACEHOLDER_MESSAGES))

    # Step 3 ‚Äî load messages
    try:
        user_messages = db.get_messages_for_today()
    except Exception as e:
        logger.error("Failed to load messages for who_is_today: %s", e)
        send_message(peer_id, "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–µ—Ä–µ–ø–∏—Å–∫—É. –ü–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ.")
        return

    # Step 4 ‚Äî call LLM
    try:
        verdict = _call_who_is_today_llm(question, user_messages)
    except Exception as e:
        logger.error("LLM call failed for who_is_today: %s", e)
        send_message(peer_id, "–ß—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –ø–µ—Ä–µ–ø–∏—Å–∫–∏. –ü–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ.")
        return

    # Step 5 ‚Äî send result
    send_message(peer_id, verdict)


def process_message(msg):
    text_raw = (msg.get("text") or "").strip()
    text = text_raw.lower()

    if msg.get("peer_id", 0) < 2000000000:
        logger.warning("Ignoring private message from peer_id=%s", msg.get("peer_id"))
        return

    logger.info("Processing message from peer_id=%s: %s", msg.get("peer_id"), text_raw)

    # Track every group chat message (best-effort ‚Äî never block command handling)
    # Use peer_id + conversation_message_id as the unique key:
    #   - msg["id"] is 0 or unreliable for group chats
    #   - conversation_message_id is a per-conversation sequential counter ‚Üí globally
    #     unique when combined with peer_id
    parts = text.split()

    user_id = msg.get("from_id")
    peer_id_val = msg.get("peer_id", "")
    conv_msg_id = msg.get("conversation_message_id", "")
    message_id = f"{peer_id_val}_{conv_msg_id}" if (peer_id_val and conv_msg_id) else ""

    # Always ensure user exists in users table (best-effort ‚Äî never block command handling).
    # Resolving the name once here also reuses it for save_message below.
    _fetched_user_name = None
    if user_id:
        try:
            _fetched_user_name = get_user_name(user_id)
            db.ensure_user(user_id, _fetched_user_name)
        except Exception as e:
            logger.warning("Failed to ensure user in users table: %s", e)

    # Only save organic chat messages ‚Äî exclude all bot commands.
    # Commands are not real chat content and would skew LLM analysis.
    _is_bot_command = (
        (parts and parts[0] == "–ø–ª–∞–Ω–∫–∞")
        or text == "—Å—Ç–∞—Ç–∞"
        or text == "–≥–∞–π–¥"
        or text.startswith("–µ–±–∞—Ç—å –≥—É—Å–µ–π")
        or text.startswith("–∫—Ç–æ —Å–µ–≥–æ–¥–Ω—è")
        or text.startswith("–æ–±—ä—è—Å–Ω–∏")
    )
    if user_id and message_id and text_raw and not _is_bot_command and _fetched_user_name:
        try:
            db.save_message(message_id, user_id, _fetched_user_name, text_raw)
        except Exception as e:
            logger.warning("Failed to save message to chat_messages: %s", e)

    if parts and parts[0] == "–ø–ª–∞–Ω–∫–∞":
        if len(parts) <= 2:
            handle_planka(msg, text_raw)

    elif text == "—Å—Ç–∞—Ç–∞":
        handle_stats(msg)

    elif text == "–≥–∞–π–¥":
        handle_guide(msg)

    elif text.startswith("–µ–±–∞—Ç—å –≥—É—Å–µ–π"):
        handle_geese(msg, text_raw)

    elif text.startswith("–∫—Ç–æ —Å–µ–≥–æ–¥–Ω—è"):
        handle_who_is_today(msg, text_raw)

    elif text.startswith("–æ–±—ä—è—Å–Ω–∏"):
        handle_explain(msg, text_raw)
